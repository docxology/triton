
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   478.536934   705.504001     208.112661
    1     384.0   657.209595   824.898310     262.003186
    2     512.0   800.450118   932.295590     300.230365
    3     640.0   883.512189   967.120223     329.020712
    4     768.0   959.154183  1035.292449     348.413885
    5     896.0  1012.439532  1078.540607     352.984898
    6    1024.0  1059.089597  1106.808683     354.087220
    7    1152.0  1095.361283  1039.215903     349.219142
    8    1280.0  1125.817115  1069.399040     350.540868
    9    1408.0  1161.191881  1113.018314     341.729369
    10   1536.0  1186.928448  1134.361637     333.248564
    11   1664.0  1220.266660  1169.614657     330.792045
    12   1792.0  1228.819140  1191.259280     325.791162
    13   1920.0  1258.284380  1189.976728     324.965995
    14   2048.0  1273.155097  1230.258558     324.321416
    15   2176.0  1235.857051   965.710201     325.371300
    16   2304.0  1254.168394  1003.316246     325.807443
    17   2432.0  1268.175727  1037.590423     326.109093
    18   2560.0  1284.960329  1067.809907     328.135820
    19   2688.0  1287.254835  1101.600462     329.625800
    20   2816.0  1312.432586  1126.785914     330.254177
    21   2944.0  1318.775534  1146.917921     332.070693
    22   3072.0  1318.784415  1174.363376     333.552400
    23   3200.0  1337.892786  1176.915092     334.207874
    24   3328.0  1341.023617  1202.867430     335.965583
    25   3456.0  1346.504269  1229.031437     336.388602
    26   3584.0  1358.675677  1245.321087     338.007456
    27   3712.0  1370.981799  1264.168957     340.516188
    28   3840.0  1369.911735  1286.188642     339.976111
    29   3968.0  1371.497518  1297.475740     340.862803
    30   4096.0  1387.880374  1319.305996     338.322667
    31   4224.0  1319.914606  1295.485346     343.387868
    32   4352.0  1334.494196  1315.099444     345.697425
    33   4480.0  1344.708111  1341.158565     345.508710
    34   4608.0  1359.765887  1355.484214     346.540275
    35   4736.0  1356.395422  1365.300075     348.283798
    36   4864.0  1359.284127  1382.954014     348.806431
    37   4992.0  1363.555540  1397.575056     350.105964
    38   5120.0  1373.456982  1405.349381     350.702871
    39   5248.0  1372.072847  1367.190960     351.813406
    40   5376.0  1371.530493  1372.806937     351.611933
    41   5504.0  1378.049123  1395.973968     353.887162
    42   5632.0  1386.727552  1411.540121     353.310181
    43   5760.0  1386.913828  1418.502757     355.308627
    44   5888.0  1383.420229  1419.460557     355.190068
    45   6016.0  1395.650565  1439.477007     356.713044
    46   6144.0  1399.076491  1452.034594     357.042494
    47   6272.0  1400.210355  1393.491463     357.659506
    48   6400.0  1403.636191  1420.896747     358.461282
    49   6528.0  1406.940038  1426.280951     359.291789
    50   6656.0  1409.197021  1430.680388     359.885101
    51   6784.0  1415.218929  1440.680221     360.528371
    52   6912.0  1414.633274  1454.323335     360.909581
    53   7040.0  1415.862687  1449.554679     361.269169
    54   7168.0  1421.293534  1468.784917     361.837457
    55   7296.0  1420.539000  1087.977235     361.921912
    56   7424.0  1425.188726  1101.178497     362.563122
    57   7552.0  1425.902090  1112.771467     362.990576
    58   7680.0  1426.170979  1125.316260     363.586760
    59   7808.0  1427.483551  1135.267884     364.480417
    60   7936.0  1427.559904  1145.158911     364.350073
    61   8064.0  1431.681085  1153.636879     364.845242
    62   8192.0  1428.117390  1155.264449     364.196320
    63   8320.0  1384.488685  1111.827831     361.770409
    64   8448.0  1384.643972  1122.929210     362.171361
    65   8576.0  1385.910844  1124.388195     363.579733
    66   8704.0  1383.556901  1131.153473     364.315877
    67   8832.0  1394.132747  1129.200192     365.200702
    68   8960.0  1387.123393  1130.934753     365.629896
    69   9088.0  1393.745489  1131.203571     365.685711
    70   9216.0  1397.583706  1130.586267     366.569065
    71   9344.0  1384.656726  1417.861264     367.252076
    72   9472.0  1394.575699  1428.934025     369.031473
    73   9600.0  1395.289558  1430.318372     368.957527
    74   9728.0  1397.826677  1441.236331     369.754570
    75   9856.0  1399.978869  1437.974079     370.200572
    76   9984.0  1394.213255  1447.051950     369.965117
    77  10112.0  1398.997390  1455.188081     371.481674
    78  10240.0  1406.555698  1461.803752     370.912819
    79  10368.0  1412.038087  1462.612440     370.094732
    80  10496.0  1406.768318  1464.188011     370.830871
    81  10624.0  1401.006963  1464.880156     370.375414
    82  10752.0  1392.610242  1469.608316     370.965084
    83  10880.0  1392.568280  1479.299851     371.185898
    84  11008.0  1418.791131  1476.701166     372.354791
    85  11136.0  1412.518006  1484.599664     373.094621
    86  11264.0  1410.826174  1487.529047     372.872806
    87  11392.0  1415.598683  1489.274480     373.983869
    88  11520.0  1409.169482  1497.259189     373.583464
    89  11648.0  1417.056426  1498.041280     374.764133
    90  11776.0  1431.266417  1500.244864     374.761893
    91  11904.0  1423.602260  1507.571355     375.387638
    92  12032.0  1410.135850  1506.780132     375.165727
    93  12160.0  1405.715414  1514.034608     375.983509
    94  12288.0  1423.673520  1422.709615     376.145489
    95  12416.0  1431.825959  1397.586152     374.805177
    96  12544.0  1439.550521  1391.978849     375.625977
    97  12672.0  1429.784996  1390.056720     375.497479




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 36.106 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
