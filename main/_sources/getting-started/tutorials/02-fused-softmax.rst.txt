
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   476.547334   700.468834     205.906513
    1     384.0   655.507150   828.829176     259.389080
    2     512.0   805.080559   915.222046     302.332447
    3     640.0   920.863301   920.202414     330.054738
    4     768.0   976.552182   988.570463     348.654737
    5     896.0  1044.758526  1041.783012     355.447949
    6    1024.0  1082.660338  1076.982136     354.515817
    7    1152.0  1094.626543  1066.394881     349.132762
    8    1280.0  1127.859686  1113.722280     349.838943
    9    1408.0  1169.219643  1138.661708     342.470244
    10   1536.0  1191.341490  1156.015897     332.739961
    11   1664.0  1209.375097  1184.688109     329.522182
    12   1792.0  1230.862036  1197.638203     325.235326
    13   1920.0  1256.518918  1225.512653     325.021394
    14   2048.0  1273.834210  1252.381240     324.632683
    15   2176.0  1233.321108   961.363410     325.694755
    16   2304.0  1261.217958  1006.648259     326.407055
    17   2432.0  1271.857234  1040.628832     326.822066
    18   2560.0  1283.386719  1070.522462     328.100853
    19   2688.0  1298.899317  1104.006945     329.063453
    20   2816.0  1309.662870  1125.907936     329.700871
    21   2944.0  1318.507270  1143.898286     331.744621
    22   3072.0  1322.558257  1174.463789     332.980063
    23   3200.0  1336.217973  1175.330724     334.384328
    24   3328.0  1350.094904  1201.926814     336.219985
    25   3456.0  1351.922906  1220.184554     336.622373
    26   3584.0  1358.278789  1250.740485     338.232863
    27   3712.0  1370.179233  1266.653995     340.417563
    28   3840.0  1369.687131  1283.416852     340.285244
    29   3968.0  1377.284142  1300.177985     341.338190
    30   4096.0  1391.445621  1322.654529     338.923123
    31   4224.0  1329.045967  1275.233136     342.807456
    32   4352.0  1342.684786  1300.540501     345.179812
    33   4480.0  1344.036211  1313.661902     345.599450
    34   4608.0  1358.599130  1335.107474     346.681786
    35   4736.0  1360.383243  1346.705990     347.539055
    36   4864.0  1368.019919  1356.818114     348.557193
    37   4992.0  1371.123060  1375.409563     350.424896
    38   5120.0  1376.672586  1384.296289     350.740047
    39   5248.0  1374.919292  1354.777794     351.646503
    40   5376.0  1375.719901  1362.520506     351.718503
    41   5504.0  1384.257734  1386.707666     353.863618
    42   5632.0  1392.327322  1388.642426     352.786807
    43   5760.0  1395.362859  1408.897362     354.507919
    44   5888.0  1391.379709  1409.244601     354.980948
    45   6016.0  1399.288800  1426.959131     356.475615
    46   6144.0  1408.403786  1426.763431     356.737085
    47   6272.0  1411.505010  1400.164112     358.205322
    48   6400.0  1413.978849  1405.539245     358.311085
    49   6528.0  1409.219037  1416.229366     359.467119
    50   6656.0  1417.445292  1421.975971     359.590515
    51   6784.0  1421.404176  1435.292564     360.243042
    52   6912.0  1425.608042  1442.407291     360.661308
    53   7040.0  1424.647970  1446.826975     361.034749
    54   7168.0  1421.878948  1457.212247     361.568241
    55   7296.0  1421.887220  1086.287801     362.319183
    56   7424.0  1428.681850  1097.226377     362.799119
    57   7552.0  1426.136633  1111.322285     363.308905
    58   7680.0  1429.700070  1123.852063     363.536623
    59   7808.0  1429.812082  1132.846242     364.867098
    60   7936.0  1436.076559  1144.509376     364.481942
    61   8064.0  1435.566156  1150.515313     365.262012
    62   8192.0  1429.299916  1153.994484     364.224388
    63   8320.0  1382.468769  1114.648972     361.872015
    64   8448.0  1384.803697  1123.665758     362.685660
    65   8576.0  1391.078678  1126.388328     363.405873
    66   8704.0  1380.605992  1133.695677     364.275801
    67   8832.0  1391.671581  1133.375885     364.947119
    68   8960.0  1385.397536  1139.027188     365.554422
    69   9088.0  1398.205754  1136.278573     366.971488
    70   9216.0  1402.880731  1142.550959     367.578305
    71   9344.0  1390.508398  1420.749861     367.546284
    72   9472.0  1401.062675  1434.100934     368.946126
    73   9600.0  1400.582265  1430.223091     368.729388
    74   9728.0  1398.069288  1438.888992     368.988687
    75   9856.0  1400.902177  1438.996876     370.182836
    76   9984.0  1394.692966  1449.013306     370.411924
    77  10112.0  1401.430014  1452.996550     371.441977
    78  10240.0  1411.586306  1461.466281     371.191454
    79  10368.0  1415.519521  1458.057945     369.407944
    80  10496.0  1409.309687  1464.615664     370.160091
    81  10624.0  1401.762690  1463.988052     370.543987
    82  10752.0  1393.622680  1473.997902     371.198620
    83  10880.0  1392.240538  1481.477557     371.288344
    84  11008.0  1421.632139  1479.135004     372.319414
    85  11136.0  1418.059866  1480.815178     372.863941
    86  11264.0  1415.718679  1483.863911     372.930452
    87  11392.0  1422.231189  1491.117846     373.587392
    88  11520.0  1410.960307  1495.074380     373.433848
    89  11648.0  1418.744295  1497.327668     374.122448
    90  11776.0  1432.116329  1504.368179     374.997198
    91  11904.0  1433.545199  1508.564768     375.739218
    92  12032.0  1414.928295  1508.026570     375.818706
    93  12160.0  1408.888109  1513.368099     375.439362
    94  12288.0  1428.846561  1418.559692     375.892457
    95  12416.0  1440.794688  1397.478682     374.323836
    96  12544.0  1444.288100  1391.427394     375.371310
    97  12672.0  1435.049548  1389.944848     375.057854




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 38.876 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
