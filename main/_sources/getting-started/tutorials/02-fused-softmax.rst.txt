
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   477.651135   692.938580     206.090497
    1     384.0   666.843264   824.598353     263.394184
    2     512.0   807.303634   933.216762     301.941888
    3     640.0   880.589674   966.874003     331.010757
    4     768.0   953.775297  1026.832432     349.346533
    5     896.0  1005.783007  1075.199995     354.284883
    6    1024.0  1049.454972  1107.309111     352.643435
    7    1152.0  1086.461879  1038.012601     347.781914
    8    1280.0  1134.451426  1064.497202     349.104163
    9    1408.0  1158.595398  1099.901437     340.016293
    10   1536.0  1193.500906  1135.022865     333.174058
    11   1664.0  1219.802242  1169.187336     330.649941
    12   1792.0  1231.195085  1186.494789     325.648195
    13   1920.0  1254.157593  1192.914794     325.136284
    14   2048.0  1269.000231  1232.332167     324.803351
    15   2176.0  1238.576261   962.320762     325.281043
    16   2304.0  1250.937628   999.480859     325.515823
    17   2432.0  1270.211889  1034.881172     326.387074
    18   2560.0  1279.820656  1072.693069     327.507918
    19   2688.0  1290.043609  1102.353600     328.799724
    20   2816.0  1302.005309  1123.514289     329.461013
    21   2944.0  1311.780306  1147.736139     331.844484
    22   3072.0  1316.642743  1173.112913     333.276934
    23   3200.0  1334.756096  1177.946368     334.182882
    24   3328.0  1338.481504  1201.903259     336.426332
    25   3456.0  1352.696649  1228.263475     336.716770
    26   3584.0  1355.560428  1249.209500     337.785781
    27   3712.0  1368.406309  1264.506657     339.979043
    28   3840.0  1369.607095  1279.630655     339.983457
    29   3968.0  1373.044644  1301.614446     341.016012
    30   4096.0  1387.124932  1314.064311     338.235613
    31   4224.0  1321.247680  1296.021420     343.119507
    32   4352.0  1339.041468  1315.200715     345.217802
    33   4480.0  1344.996969  1337.713267     345.646825
    34   4608.0  1353.163045  1353.760176     346.829744
    35   4736.0  1352.060759  1369.568411     347.820091
    36   4864.0  1362.440222  1383.408315     349.262354
    37   4992.0  1359.591058  1391.636205     350.000029
    38   5120.0  1367.796937  1409.327687     350.421452
    39   5248.0  1368.006573  1364.196566     351.503570
    40   5376.0  1371.731690  1381.262098     351.579580
    41   5504.0  1373.533935  1393.035725     353.793009
    42   5632.0  1384.930829  1401.196890     353.161961
    43   5760.0  1388.498437  1412.460884     355.125892
    44   5888.0  1390.554910  1431.174691     354.739602
    45   6016.0  1391.866446  1430.089160     356.924324
    46   6144.0  1400.548264  1443.960787     356.968407
    47   6272.0  1400.520218  1409.214352     357.571772
    48   6400.0  1406.020690  1411.061369     358.348239
    49   6528.0  1412.083043  1418.833831     359.340998
    50   6656.0  1406.221457  1433.002888     359.232559
    51   6784.0  1408.967856  1441.259366     359.921437
    52   6912.0  1421.209036  1450.977877     360.688877
    53   7040.0  1411.995090  1453.992465     361.030230
    54   7168.0  1417.312335  1458.230730     361.828324
    55   7296.0  1416.735994  1085.936414     362.781476
    56   7424.0  1422.306721  1101.740998     362.694698
    57   7552.0  1420.068542  1112.560357     363.636910
    58   7680.0  1424.967199  1125.586273     363.545738
    59   7808.0  1421.120035  1135.512100     364.076418
    60   7936.0  1426.080355  1144.398349     364.582044
    61   8064.0  1427.807740  1152.982303     364.904076
    62   8192.0  1426.759697  1154.896468     363.791142
    63   8320.0  1382.691427  1113.401827     361.645324
    64   8448.0  1384.392689  1124.346922     362.530904
    65   8576.0  1385.490103  1124.232351     363.490553
    66   8704.0  1383.451033  1129.179774     364.298063
    67   8832.0  1396.805750  1128.593240     365.129486
    68   8960.0  1385.687197  1132.263504     365.838722
    69   9088.0  1393.308614  1130.046338     366.768901
    70   9216.0  1402.768136  1130.958307     366.969931
    71   9344.0  1387.953618  1422.236290     367.546284
    72   9472.0  1397.907587  1426.753242     368.780037
    73   9600.0  1397.858680  1427.780722     368.363169
    74   9728.0  1396.955743  1442.045004     369.772316
    75   9856.0  1397.173326  1436.022280     369.547914
    76   9984.0  1393.685482  1449.567175     370.768762
    77  10112.0  1405.340704  1452.881725     371.113601
    78  10240.0  1402.107125  1464.093715     371.429674
    79  10368.0  1407.389432  1460.983185     370.174654
    80  10496.0  1408.844646  1463.507249     370.773028
    81  10624.0  1403.671300  1464.188035     370.433065
    82  10752.0  1393.068487  1474.735550     370.865192
    83  10880.0  1393.662427  1476.510534     372.124840
    84  11008.0  1418.276272  1476.564945     372.177976
    85  11136.0  1414.513761  1484.020803     373.067990
    86  11264.0  1410.548662  1487.879999     372.890542
    87  11392.0  1412.131943  1489.210407     374.443071
    88  11520.0  1407.553225  1493.679089     373.838968
    89  11648.0  1418.542583  1499.224233     374.281558
    90  11776.0  1426.673401  1499.153014     374.792955
    91  11904.0  1424.220060  1508.901926     375.249526
    92  12032.0  1410.952364  1510.730391     375.809866
    93  12160.0  1407.991539  1516.282658     375.824085
    94  12288.0  1424.203093  1421.700298     375.794886
    95  12416.0  1427.251591  1397.853426     374.566447
    96  12544.0  1438.925895  1394.122004     375.327437
    97  12672.0  1428.803965  1395.032349     375.268747




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 36.415 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
