
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   480.634318   692.172491     208.643438
    1     384.0   660.992748   823.443624     264.840803
    2     512.0   804.880473   917.003537     300.323883
    3     640.0   918.428844   929.459747     328.650956
    4     768.0   983.177271   979.164139     348.868275
    5     896.0  1038.496104  1025.109417     354.113789
    6    1024.0  1083.028501  1077.352507     353.214264
    7    1152.0  1100.755692  1077.136498     349.879381
    8    1280.0  1141.749568  1112.850196     349.465844
    9    1408.0  1162.267740  1142.112198     341.968090
    10   1536.0  1197.785919  1166.096924     334.153038
    11   1664.0  1218.212625  1185.075161     329.732877
    12   1792.0  1238.514183  1202.444554     326.408284
    13   1920.0  1251.376932  1217.564675     324.434820
    14   2048.0  1275.489652  1251.355697     325.202870
    15   2176.0  1239.060073   963.967965     325.611102
    16   2304.0  1251.012964  1004.034755     326.224405
    17   2432.0  1268.954300  1040.160474     326.636862
    18   2560.0  1290.078747  1070.754709     328.985019
    19   2688.0  1299.135919  1096.801832     329.058917
    20   2816.0  1315.343224  1126.912261     329.962927
    21   2944.0  1314.515972  1148.320535     332.052370
    22   3072.0  1325.836993  1169.438893     333.149064
    23   3200.0  1343.657363  1174.703127     334.804380
    24   3328.0  1347.587363  1204.215208     335.903455
    25   3456.0  1353.968369  1221.146923     336.720940
    26   3584.0  1364.845899  1243.332429     338.097178
    27   3712.0  1368.678856  1267.755520     340.733874
    28   3840.0  1372.603292  1285.773605     340.058371
    29   3968.0  1372.613492  1303.356916     341.405114
    30   4096.0  1388.837430  1316.066555     338.956457
    31   4224.0  1332.194198  1279.787060     343.220857
    32   4352.0  1338.390584  1300.967197     345.664610
    33   4480.0  1350.422862  1320.734126     345.650438
    34   4608.0  1356.254065  1336.306445     346.667404
    35   4736.0  1358.207179  1347.000811     347.845953
    36   4864.0  1366.273302  1360.829334     348.673834
    37   4992.0  1368.541622  1370.769519     349.903704
    38   5120.0  1374.956740  1384.099724     350.617907
    39   5248.0  1371.266162  1357.832647     351.897926
    40   5376.0  1382.242993  1370.054272     351.934532
    41   5504.0  1377.607825  1376.664649     354.476761
    42   5632.0  1391.416873  1396.420609     353.478079
    43   5760.0  1389.826252  1410.487122     355.284218
    44   5888.0  1391.310286  1409.596031     355.106392
    45   6016.0  1407.009472  1420.464116     356.308545
    46   6144.0  1410.216224  1429.318958     356.843456
    47   6272.0  1405.588924  1399.586371     357.636413
    48   6400.0  1413.137920  1406.604957     358.161666
    49   6528.0  1418.264211  1418.797224     359.559466
    50   6656.0  1412.496788  1430.491655     359.585915
    51   6784.0  1416.721071  1443.279073     360.155694
    52   6912.0  1421.688081  1442.417269     361.006222
    53   7040.0  1421.909996  1456.617837     360.892527
    54   7168.0  1420.832838  1456.413001     361.440619
    55   7296.0  1426.737062  1087.651809     362.257444
    56   7424.0  1427.572981  1100.359415     363.058160
    57   7552.0  1430.429868  1109.841943     362.949687
    58   7680.0  1429.763949  1121.427514     363.709881
    59   7808.0  1430.352999  1133.554052     364.484961
    60   7936.0  1430.405556  1144.064063     364.900917
    61   8064.0  1436.504603  1148.787425     365.443512
    62   8192.0  1428.338798  1153.806706     364.308297
    63   8320.0  1384.216018  1115.024149     361.877695
    64   8448.0  1386.672199  1124.238014     362.514444
    65   8576.0  1391.251637  1128.736659     363.602033
    66   8704.0  1383.847892  1133.499370     364.235730
    67   8832.0  1394.765104  1134.322629     365.000476
    68   8960.0  1386.190185  1140.021333     365.821965
    69   9088.0  1397.259863  1136.782563     366.755559
    70   9216.0  1404.187352  1144.169152     367.576911
    71   9344.0  1389.483211  1422.994312     367.702913
    72   9472.0  1395.972977  1430.254804     369.086422
    73   9600.0  1402.293001  1427.634475     369.383244
    74   9728.0  1400.717995  1438.471106     369.240363
    75   9856.0  1395.588446  1437.814035     369.886025
    76   9984.0  1393.635240  1452.065293     370.764325
    77  10112.0  1406.862487  1455.450471     371.353169
    78  10240.0  1413.912826  1468.019103     371.617886
    79  10368.0  1416.224147  1460.858852     370.285712
    80  10496.0  1407.587361  1462.008117     370.115754
    81  10624.0  1404.968048  1465.050924     370.393151
    82  10752.0  1393.978614  1468.039896     371.056283
    83  10880.0  1393.180812  1477.481288     372.359211
    84  11008.0  1424.500350  1474.506006     372.279624
    85  11136.0  1418.928642  1483.593841     373.005864
    86  11264.0  1413.545058  1487.885854     373.054675
    87  11392.0  1420.531907  1490.121244     374.042952
    88  11520.0  1415.445082  1491.272107     373.706767
    89  11648.0  1418.114566  1497.938448     374.179887
    90  11776.0  1435.646432  1499.166554     374.646567
    91  11904.0  1429.881541  1505.472842     375.549590
    92  12032.0  1413.912196  1508.008580     376.075223
    93  12160.0  1406.622597  1514.233531     375.930354
    94  12288.0  1426.136055  1417.846141     376.096633
    95  12416.0  1441.983781  1395.651247     375.014819
    96  12544.0  1448.864818  1394.945425     375.573258
    97  12672.0  1435.609623  1394.974473     375.369883




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 39.858 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
