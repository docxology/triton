
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   478.879516   694.331422     206.553764
    1     384.0   665.686827   829.938236     263.583763
    2     512.0   800.863740   939.561627     302.025985
    3     640.0   889.203098   971.257863     331.140872
    4     768.0   955.724031  1022.520655     350.781339
    5     896.0  1011.551315  1079.402980     355.063703
    6    1024.0  1050.963003  1110.533389     352.817157
    7    1152.0  1100.731423  1039.273850     349.295755
    8    1280.0  1137.600555  1071.457500     350.045467
    9    1408.0  1159.706087  1114.173879     341.114339
    10   1536.0  1197.327413  1136.513749     334.099955
    11   1664.0  1216.292975  1163.812814     329.577094
    12   1792.0  1236.405382  1196.785463     326.038880
    13   1920.0  1251.565797  1190.300129     324.564181
    14   2048.0  1277.027029  1230.627511     324.927673
    15   2176.0  1238.299369   967.202254     325.463223
    16   2304.0  1256.409280   999.113611     326.103832
    17   2432.0  1266.744169  1034.557308     326.422068
    18   2560.0  1287.305617  1068.459840     327.714048
    19   2688.0  1290.498694  1100.972809     329.792963
    20   2816.0  1310.554004  1125.693257     329.796665
    21   2944.0  1316.384254  1144.271390     331.654030
    22   3072.0  1318.463337  1173.191298     333.627033
    23   3200.0  1336.764539  1177.448067     334.826433
    24   3328.0  1338.632160  1207.671801     336.291069
    25   3456.0  1346.625122  1228.701528     337.236902
    26   3584.0  1359.719071  1244.737886     338.328276
    27   3712.0  1368.542801  1266.892800     340.277922
    28   3840.0  1365.530359  1285.468789     339.999120
    29   3968.0  1371.192613  1301.717354     341.328265
    30   4096.0  1384.324668  1315.059399     338.846535
    31   4224.0  1322.976840  1291.048069     343.403541
    32   4352.0  1339.836796  1318.297803     345.105216
    33   4480.0  1341.507337  1336.397672     345.721885
    34   4608.0  1352.400535  1355.876756     346.910770
    35   4736.0  1346.178921  1368.050594     348.071411
    36   4864.0  1360.067935  1384.306816     348.791035
    37   4992.0  1363.901725  1393.344903     350.071301
    38   5120.0  1367.091014  1405.268549     350.554228
    39   5248.0  1362.996915  1366.453341     351.609394
    40   5376.0  1371.245836  1383.399066     351.614455
    41   5504.0  1376.601001  1386.590649     353.976655
    42   5632.0  1387.611515  1398.059468     353.549018
    43   5760.0  1392.157738  1422.209750     355.027378
    44   5888.0  1384.210646  1432.247263     355.008816
    45   6016.0  1399.044513  1432.054541     356.787535
    46   6144.0  1401.059617  1452.053192     357.116613
    47   6272.0  1398.140808  1407.455493     357.599475
    48   6400.0  1404.857828  1414.113760     358.376110
    49   6528.0  1406.629415  1416.054368     359.360978
    50   6656.0  1407.705744  1440.708064     359.567521
    51   6784.0  1414.708830  1441.807485     360.385651
    52   6912.0  1420.061217  1442.544066     360.863576
    53   7040.0  1412.191713  1455.701450     360.957318
    54   7168.0  1417.731602  1457.898318     361.800928
    55   7296.0  1421.530631  1085.953028     362.200368
    56   7424.0  1421.193079  1101.944833     363.058161
    57   7552.0  1420.526254  1114.701826     363.327112
    58   7680.0  1426.649967  1124.471695     363.418173
    59   7808.0  1423.822758  1135.022515     364.152932
    60   7936.0  1430.498862  1145.853870     364.616144
    61   8064.0  1430.047268  1153.834089     365.221197
    62   8192.0  1432.140616  1155.306877     364.219869
    63   8320.0  1381.444472  1113.146495     361.858721
    64   8448.0  1383.580672  1122.728403     362.581352
    65   8576.0  1389.393836  1123.689728     363.396962
    66   8704.0  1382.676604  1130.325682     364.213475
    67   8832.0  1395.288425  1129.999897     364.742730
    68   8960.0  1385.496795  1131.139101     365.780939
    69   9088.0  1395.079748  1131.709833     366.773349
    70   9216.0  1402.781647  1130.022388     367.347980
    71   9344.0  1386.286377  1420.615521     367.703702
    72   9472.0  1397.944884  1430.682123     368.256175
    73   9600.0  1399.978729  1431.324292     369.155340
    74   9728.0  1401.383979  1434.996385     370.003169
    75   9856.0  1399.600759  1440.350864     369.561307
    76   9984.0  1391.692220  1450.320393     370.038282
    77  10112.0  1404.584355  1452.474126     371.837686
    78  10240.0  1407.813728  1464.038636     370.959981
    79  10368.0  1412.755277  1458.467427     369.452175
    80  10496.0  1405.396918  1467.424458     370.732991
    81  10624.0  1403.505020  1463.068786     370.517359
    82  10752.0  1391.352172  1469.733372     371.136334
    83  10880.0  1389.761208  1477.682104     371.604486
    84  11008.0  1417.425474  1475.768426     372.425563
    85  11136.0  1413.718100  1483.544026     372.872808
    86  11264.0  1412.094848  1485.559328     373.001426
    87  11392.0  1415.465860  1485.921117     373.556590
    88  11520.0  1411.627536  1493.566842     373.649509
    89  11648.0  1417.484584  1495.226617     374.524904
    90  11776.0  1429.246675  1503.858512     374.668740
    91  11904.0  1424.933160  1509.613840     375.342542
    92  12032.0  1416.663669  1509.120655     375.372846
    93  12160.0  1410.040848  1514.347501     376.147504
    94  12288.0  1422.844454  1419.848494     376.074428
    95  12416.0  1427.849130  1396.320021     374.870661
    96  12544.0  1438.833413  1394.617133     375.520554
    97  12672.0  1428.081276  1389.719928     374.785802




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.164 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
