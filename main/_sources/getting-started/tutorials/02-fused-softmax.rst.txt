
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   470.500443   692.076963     207.053117
    1     384.0   656.704797   833.828570     262.143994
    2     512.0   798.924779   941.627293     302.149014
    3     640.0   887.069311   955.287751     330.333662
    4     768.0   966.002911  1023.410813     350.480124
    5     896.0  1023.613140  1080.266732     355.298198
    6    1024.0  1061.092251  1099.477446     354.257846
    7    1152.0  1095.193232  1041.246962     350.030772
    8    1280.0  1139.375005  1078.001408     349.159313
    9    1408.0  1163.613709  1102.990471     340.205763
    10   1536.0  1196.665235  1134.615864     333.901021
    11   1664.0  1209.449560  1161.227293     329.751516
    12   1792.0  1237.248392  1195.015196     325.756261
    13   1920.0  1262.739456  1192.741556     325.073107
    14   2048.0  1269.694369  1221.404776     325.002425
    15   2176.0  1233.561895   962.118398     326.002436
    16   2304.0  1250.279145  1004.653173     326.009107
    17   2432.0  1277.368933  1036.547657     326.829359
    18   2560.0  1286.764983  1071.223122     328.381288
    19   2688.0  1295.108461  1102.357327     329.639767
    20   2816.0  1305.622993  1127.875447     330.198325
    21   2944.0  1319.039095  1145.718855     331.304371
    22   3072.0  1321.027251  1170.581524     333.829489
    23   3200.0  1335.030284  1174.766249     334.779963
    24   3328.0  1344.893342  1202.935209     336.657533
    25   3456.0  1352.370933  1227.341283     337.092082
    26   3584.0  1359.535128  1245.831965     338.445119
    27   3712.0  1365.608019  1264.269349     340.685701
    28   3840.0  1371.516709  1286.279589     340.060590
    29   3968.0  1371.427539  1302.569091     340.978020
    30   4096.0  1380.777462  1320.931728     338.956834
    31   4224.0  1321.858572  1295.652741     343.140642
    32   4352.0  1336.790286  1323.043813     345.271294
    33   4480.0  1335.697019  1341.269019     346.158663
    34   4608.0  1354.850117  1353.467819     347.004478
    35   4736.0  1354.733526  1366.759052     348.078259
    36   4864.0  1365.873320  1379.858134     349.069187
    37   4992.0  1361.651229  1388.398515     350.112978
    38   5120.0  1370.082137  1404.242525     350.678279
    39   5248.0  1367.723237  1359.444740     352.095478
    40   5376.0  1369.340629  1380.288278     351.770980
    41   5504.0  1377.546459  1386.988016     353.764772
    42   5632.0  1389.341234  1401.070705     353.179192
    43   5760.0  1385.328806  1409.279616     355.172635
    44   5888.0  1388.056703  1432.852816     355.008817
    45   6016.0  1395.137788  1440.862949     356.568807
    46   6144.0  1405.944724  1444.959590     356.968408
    47   6272.0  1400.534412  1405.745184     357.599476
    48   6400.0  1408.903272  1416.874063     359.001857
    49   6528.0  1413.308691  1420.362718     359.573531
    50   6656.0  1404.772751  1436.314645     359.604311
    51   6784.0  1412.538158  1438.830175     360.652770
    52   6912.0  1418.743239  1448.598521     361.125947
    53   7040.0  1416.246065  1459.403063     360.796194
    54   7168.0  1421.076761  1464.229902     361.518094
    55   7296.0  1415.362400  1086.373914     362.184931
    56   7424.0  1425.060415  1100.710517     362.971773
    57   7552.0  1423.088119  1114.655667     363.436392
    58   7680.0  1426.438817  1125.365668     363.487037
    59   7808.0  1427.097329  1136.031834     364.466786
    60   7936.0  1430.758532  1145.443225     364.477391
    61   8064.0  1425.082661  1151.688399     365.012742
    62   8192.0  1425.550363  1155.802996     364.459494
    63   8320.0  1384.771652  1114.259186     361.868752
    64   8448.0  1384.728160  1122.687420     362.300498
    65   8576.0  1385.759064  1122.503578     363.624339
    66   8704.0  1384.511249  1131.693993     364.387147
    67   8832.0  1397.195283  1128.073412     365.374411
    68   8960.0  1384.015001  1132.400433     365.865396
    69   9088.0  1395.189416  1130.212422     367.080504
    70   9216.0  1402.712407  1129.466723     367.397333
    71   9344.0  1389.840604  1422.467790     367.952127
    72   9472.0  1400.133508  1425.839426     368.251347
    73   9600.0  1400.549250  1429.987407     368.416001
    74   9728.0  1397.876893  1442.281082     369.701344
    75   9856.0  1400.020636  1440.737189     370.134074
    76   9984.0  1392.305126  1445.296276     370.063322
    77  10112.0  1404.235615  1451.099581     371.180117
    78  10240.0  1404.441931  1465.504085     371.752438
    79  10368.0  1412.271125  1458.635910     370.419073
    80  10496.0  1406.848274  1464.935903     370.044837
    81  10624.0  1405.288090  1468.428699     371.308922
    82  10752.0  1392.521593  1473.017382     371.733353
    83  10880.0  1390.095418  1474.614636     371.789274
    84  11008.0  1416.199409  1474.145876     372.328257
    85  11136.0  1418.159625  1482.146235     373.143456
    86  11264.0  1403.197851  1487.892277     373.054674
    87  11392.0  1414.885471  1491.617092     373.701844
    88  11520.0  1412.125243  1495.148936     374.536770
    89  11648.0  1421.904686  1499.299924     375.185744
    90  11776.0  1425.546649  1502.042798     374.775205
    91  11904.0  1429.503330  1508.296580     375.401010
    92  12032.0  1408.331725  1509.800172     376.044246
    93  12160.0  1407.942109  1512.937553     376.001232
    94  12288.0  1421.223909  1419.708324     376.252132
    95  12416.0  1430.561627  1394.378945     374.738674
    96  12544.0  1435.135282  1393.627901     375.419582
    97  12672.0  1428.730985  1391.392663     375.651593




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.179 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
