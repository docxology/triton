
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   461.443611   686.002159     204.126004
    1     384.0   651.992913   818.931057     262.351985
    2     512.0   804.712445   943.622458     300.513129
    3     640.0   866.168275   959.845388     329.269296
    4     768.0   964.193629  1031.487561     348.119086
    5     896.0  1011.741515  1079.340323     354.462653
    6    1024.0  1059.248653  1101.294766     352.950943
    7    1152.0  1098.418590  1029.924598     348.656261
    8    1280.0  1136.302798  1077.248021     349.148942
    9    1408.0  1166.678201  1100.010348     339.752758
    10   1536.0  1184.385556  1133.646738     332.677355
    11   1664.0  1212.785717  1170.644456     329.785829
    12   1792.0  1230.614707  1187.276348     325.413118
    13   1920.0  1262.611407  1197.825934     324.487563
    14   2048.0  1268.936565  1229.448056     324.505216
    15   2176.0  1229.711154   960.616452     325.255065
    16   2304.0  1250.720312  1003.071400     325.941645
    17   2432.0  1268.109092  1035.005446     326.468771
    18   2560.0  1285.403889  1065.878151     327.838266
    19   2688.0  1287.990413  1095.142834     328.888164
    20   2816.0  1307.098573  1119.547622     328.886579
    21   2944.0  1319.740634  1141.979550     331.641197
    22   3072.0  1316.799152  1172.547583     332.930048
    23   3200.0  1333.488843  1169.877702     334.302007
    24   3328.0  1339.780933  1205.823409     336.062534
    25   3456.0  1347.023644  1227.875028     336.964236
    26   3584.0  1362.602998  1248.033054     337.926476
    27   3712.0  1368.327218  1268.498538     340.438766
    28   3840.0  1367.106774  1281.869376     339.678412
    29   3968.0  1374.277389  1300.050885     341.037932
    30   4096.0  1385.229628  1319.803974     338.057251
    31   4224.0  1328.411928  1293.136210     343.085670
    32   4352.0  1331.522750  1314.413891     345.010325
    33   4480.0  1337.520786  1339.813086     345.582879
    34   4608.0  1355.882214  1353.143760     347.111793
    35   4736.0  1349.629549  1368.890087     347.472993
    36   4864.0  1360.393011  1380.453676     348.629482
    37   4992.0  1360.137004  1387.433052     350.330107
    38   5120.0  1369.092780  1410.241665     351.114651
    39   5248.0  1370.840296  1368.273502     351.597596
    40   5376.0  1373.848793  1373.117420     351.770766
    41   5504.0  1370.723668  1396.985948     353.722084
    42   5632.0  1389.025865  1407.684885     353.175895
    43   5760.0  1384.418089  1411.922040     354.967708
    44   5888.0  1381.811404  1428.665465     354.915939
    45   6016.0  1395.515422  1433.235318     356.489759
    46   6144.0  1401.823652  1438.919718     356.797032
    47   6272.0  1402.728255  1393.806829     357.440244
    48   6400.0  1405.460229  1412.791879     358.617828
    49   6528.0  1406.961103  1416.677985     359.220200
    50   6656.0  1412.473262  1437.073605     359.622711
    51   6784.0  1412.398738  1436.373067     360.017893
    52   6912.0  1417.846166  1449.465688     360.619961
    53   7040.0  1417.977499  1456.831721     361.039415
    54   7168.0  1412.256733  1457.612196     361.873990
    55   7296.0  1421.509378  1086.971876     362.067934
    56   7424.0  1424.256874  1101.881220     362.844539
    57   7552.0  1421.524204  1114.269842     363.440948
    58   7680.0  1429.185844  1126.317710     363.717587
    59   7808.0  1423.478146  1137.001148     364.425895
    60   7936.0  1425.698914  1147.592765     364.550187
    61   8064.0  1430.460106  1153.916808     365.103345
    62   8192.0  1429.224419  1156.693613     363.903868
    63   8320.0  1383.981813  1113.501728     361.841000
    64   8448.0  1381.137474  1123.131638     362.634898
    65   8576.0  1382.594689  1122.663917     363.419854
    66   8704.0  1382.042628  1132.395370     364.378237
    67   8832.0  1390.719861  1128.230398     364.840452
    68   8960.0  1386.051476  1130.121126     365.763161
    69   9088.0  1394.323285  1131.305789     366.715537
    70   9216.0  1400.611979  1132.371502     366.756776
    71   9344.0  1385.437142  1422.115241     367.716345
    72   9472.0  1398.353293  1426.736380     368.928164
    73   9600.0  1400.301527  1431.005402     368.483671
    74   9728.0  1400.799049  1440.981496     369.878827
    75   9856.0  1395.093580  1440.095934     370.035124
    76   9984.0  1390.736219  1445.217836     369.933881
    77  10112.0  1404.625077  1456.878861     370.896482
    78  10240.0  1405.309470  1464.758048     371.599104
    79  10368.0  1411.762280  1457.229394     369.421214
    80  10496.0  1406.736819  1465.956173     370.089158
    81  10624.0  1402.716481  1466.792536     370.934953
    82  10752.0  1396.522890  1474.602455     371.599525
    83  10880.0  1392.546782  1477.457891     371.809596
    84  11008.0  1414.415709  1473.197951     372.558335
    85  11136.0  1417.389960  1481.234623     372.948196
    86  11264.0  1407.114970  1485.701397     372.903845
    87  11392.0  1414.686583  1486.326663     373.820771
    88  11520.0  1409.665495  1496.271073     373.548250
    89  11648.0  1415.551352  1497.575237     374.157794
    90  11776.0  1424.105288  1503.209961     375.010524
    91  11904.0  1423.827023  1509.472628     376.335802
    92  12032.0  1414.990450  1509.010385     375.650831
    93  12160.0  1410.012617  1515.767261     375.943643
    94  12288.0  1428.459713  1424.226528     376.154375
    95  12416.0  1429.764290  1396.268945     374.641501
    96  12544.0  1434.751756  1392.810552     375.647946
    97  12672.0  1430.279139  1390.408267     375.400674




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.964 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
