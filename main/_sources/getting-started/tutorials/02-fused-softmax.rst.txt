
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   467.766836   688.984200     204.557409
    1     384.0   653.509893   823.988529     259.495626
    2     512.0   813.670879   933.916018     302.397395
    3     640.0   884.987216   959.050599     330.510602
    4     768.0   961.109976  1023.177265     350.629354
    5     896.0  1009.085197  1078.367462     354.823799
    6    1024.0  1052.307264  1101.579543     353.798118
    7    1152.0  1090.919835  1032.276551     348.507920
    8    1280.0  1133.305725  1069.677887     349.742697
    9    1408.0  1163.386952  1110.743824     340.916304
    10   1536.0  1191.688415  1134.090224     332.063965
    11   1664.0  1207.332572  1168.709393     328.995241
    12   1792.0  1236.889092  1193.771324     324.983648
    13   1920.0  1263.653323  1191.113952     324.352072
    14   2048.0  1269.640106  1224.070063     324.919407
    15   2176.0  1232.812420   966.987295     325.931967
    16   2304.0  1255.980259  1002.630528     326.543936
    17   2432.0  1270.263416  1033.808768     327.358774
    18   2560.0  1286.685575  1066.307207     328.165964
    19   2688.0  1289.051919  1099.597352     329.348318
    20   2816.0  1306.788495  1125.209160     329.743884
    21   2944.0  1310.321030  1142.485392     330.841938
    22   3072.0  1320.411460  1172.195998     333.096712
    23   3200.0  1334.191243  1173.168699     335.112193
    24   3328.0  1342.193552  1206.644348     336.193452
    25   3456.0  1352.225679  1226.095606     337.275127
    26   3584.0  1358.694377  1245.235049     338.191146
    27   3712.0  1361.369186  1264.401204     340.848875
    28   3840.0  1367.826748  1281.681201     340.194377
    29   3968.0  1375.308593  1302.470160     341.116415
    30   4096.0  1384.889320  1316.781947     339.075639
    31   4224.0  1331.034564  1295.073899     342.517193
    32   4352.0  1340.289463  1318.339733     344.864083
    33   4480.0  1337.607026  1334.978045     345.086600
    34   4608.0  1355.019697  1353.489962     347.113836
    35   4736.0  1353.820323  1362.122414     348.195291
    36   4864.0  1360.844102  1384.552441     349.157352
    37   4992.0  1361.317636  1398.775521     350.008766
    38   5120.0  1368.689880  1410.901981     351.156009
    39   5248.0  1368.642284  1364.118809     351.377367
    40   5376.0  1375.152531  1380.404989     351.476965
    41   5504.0  1376.098038  1397.246059     353.885737
    42   5632.0  1390.502081  1407.173900     352.716829
    43   5760.0  1385.550896  1421.783435     354.781791
    44   5888.0  1381.895748  1428.454299     354.809189
    45   6016.0  1392.127505  1439.302165     356.647891
    46   6144.0  1400.773810  1439.511048     357.079549
    47   6272.0  1403.369185  1409.312992     357.765729
    48   6400.0  1403.949479  1413.181650     358.742916
    49   6528.0  1407.941732  1416.568098     359.130453
    50   6656.0  1407.466516  1436.784424     359.484764
    51   6784.0  1411.436250  1442.723664     360.137309
    52   6912.0  1420.613826  1444.392340     360.891720
    53   7040.0  1413.474318  1462.888107     360.857159
    54   7168.0  1415.491162  1462.893385     361.745785
    55   7296.0  1420.941402  1086.213483     362.369476
    56   7424.0  1427.058689  1102.162103     363.344902
    57   7552.0  1418.781393  1113.285132     363.568526
    58   7680.0  1423.031293  1125.174401     363.780926
    59   7808.0  1427.632866  1135.354547     364.439523
    60   7936.0  1433.064443  1145.934246     364.647736
    61   8064.0  1429.248401  1153.604434     365.166795
    62   8192.0  1430.019221  1157.319709     364.256020
    63   8320.0  1378.196500  1112.596963     361.689987
    64   8448.0  1386.470623  1122.511571     362.287134
    65   8576.0  1386.257095  1122.346628     363.134273
    66   8704.0  1382.906471  1132.171057     364.413882
    67   8832.0  1397.831224  1129.755172     365.276400
    68   8960.0  1383.514624  1133.372046     365.985484
    69   9088.0  1398.204961  1131.616621     366.129552
    70   9216.0  1401.906079  1132.975079     367.645039
    71   9344.0  1391.448318  1423.278691     367.693959
    72   9472.0  1399.941502  1429.810502     368.872952
    73   9600.0  1401.847146  1429.891283     369.612821
    74   9728.0  1400.191433  1438.434011     369.253770
    75   9856.0  1396.806332  1440.673412     369.918793
    76   9984.0  1389.298355  1443.486807     369.956190
    77  10112.0  1400.995384  1452.012087     370.927484
    78  10240.0  1406.401814  1467.610438     371.683403
    79  10368.0  1410.707602  1459.138827     369.487570
    80  10496.0  1406.980131  1463.953824     369.960658
    81  10624.0  1399.020134  1464.491607     371.291097
    82  10752.0  1395.686862  1469.167786     371.109646
    83  10880.0  1389.652557  1480.814677     371.379469
    84  11008.0  1418.488745  1477.573946     372.381327
    85  11136.0  1413.529596  1482.514972     372.903846
    86  11264.0  1406.499399  1483.925765     372.682259
    87  11392.0  1417.944199  1490.166508     373.788753
    88  11520.0  1409.536659  1492.913697     373.759637
    89  11648.0  1414.760711  1501.151835     374.604613
    90  11776.0  1424.516134  1501.690500     374.863970
    91  11904.0  1423.606040  1507.704265     375.209444
    92  12032.0  1409.600057  1507.240686     376.035397
    93  12160.0  1411.803733  1513.411133     375.819657
    94  12288.0  1421.037988  1422.338482     376.283249
    95  12416.0  1429.016301  1396.593587     374.517902
    96  12544.0  1437.534510  1389.684898     375.077556
    97  12672.0  1430.053183  1389.207458     375.409473




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 36.445 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
