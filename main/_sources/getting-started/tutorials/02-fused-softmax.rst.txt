
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   477.589617   693.092569     207.387486
    1     384.0   656.234990   831.702720     263.245575
    2     512.0   801.729887   934.507398     304.215416
    3     640.0   879.650963   918.532997     331.620601
    4     768.0   964.784595   993.232331     350.705330
    5     896.0  1025.181078  1031.590995     355.654149
    6    1024.0  1063.065768  1081.413779     355.744262
    7    1152.0  1091.501631  1078.022898     350.480353
    8    1280.0  1125.113836  1113.191301     348.780770
    9    1408.0  1161.033116  1132.998455     341.957403
    10   1536.0  1200.209566  1168.459669     333.245151
    11   1664.0  1218.197767  1192.188895     329.907677
    12   1792.0  1238.617590  1200.806676     326.113865
    13   1920.0  1255.516543  1228.209304     325.072847
    14   2048.0  1268.188626  1242.875444     324.740119
    15   2176.0  1228.541883   962.577697     325.869036
    16   2304.0  1251.227656  1001.981114     325.871036
    17   2432.0  1276.640429  1037.622899     327.420098
    18   2560.0  1286.359389  1069.052093     328.605571
    19   2688.0  1295.275485  1094.941784     328.983629
    20   2816.0  1309.417184  1125.106802     329.712307
    21   2944.0  1314.933126  1149.695123     331.083425
    22   3072.0  1322.867056  1168.372803     333.398381
    23   3200.0  1335.525962  1171.430096     335.230832
    24   3328.0  1341.927170  1205.895449     336.265170
    25   3456.0  1353.196842  1220.373841     336.865258
    26   3584.0  1358.813283  1243.771293     338.204129
    27   3712.0  1369.159229  1267.848919     340.626416
    28   3840.0  1367.153019  1284.448805     340.695556
    29   3968.0  1374.903947  1302.267505     341.085918
    30   4096.0  1386.135771  1317.205769     338.826627
    31   4224.0  1328.470408  1278.754646     343.296337
    32   4352.0  1336.739979  1301.024760     345.688713
    33   4480.0  1341.244660  1315.947200     345.756366
    34   4608.0  1351.002636  1332.498457     347.403396
    35   4736.0  1347.726154  1346.120903     347.909815
    36   4864.0  1359.954728  1360.737513     349.133619
    37   4992.0  1361.614757  1371.029653     349.963433
    38   5120.0  1371.694813  1384.658505     350.858743
    39   5248.0  1369.854630  1350.722357     352.109949
    40   5376.0  1372.829931  1374.063737     352.180257
    41   5504.0  1374.433401  1382.886621     354.012085
    42   5632.0  1389.065906  1399.929735     353.347842
    43   5760.0  1385.273944  1399.169486     355.316830
    44   5888.0  1388.939834  1405.458458     355.087803
    45   6016.0  1394.559587  1427.106247     356.854923
    46   6144.0  1402.085977  1423.326827     356.787716
    47   6272.0  1406.985767  1393.863936     358.105936
    48   6400.0  1403.741240  1406.764609     359.015739
    49   6528.0  1412.818655  1409.491984     359.387481
    50   6656.0  1411.974600  1438.187063     359.866674
    51   6784.0  1410.870131  1428.470789     360.436282
    52   6912.0  1415.838997  1444.335960     360.992412
    53   7040.0  1416.271769  1443.929035     360.764097
    54   7168.0  1416.135120  1460.320899     361.673142
    55   7296.0  1423.182965  1085.078186     362.511281
    56   7424.0  1426.201960  1099.678960     363.049067
    57   7552.0  1420.682421  1110.104350     363.604996
    58   7680.0  1423.814299  1121.626911     363.960943
    59   7808.0  1430.473291  1131.716612     364.662283
    60   7936.0  1427.635199  1142.522544     365.004672
    61   8064.0  1432.780608  1150.195881     364.931237
    62   8192.0  1432.791789  1153.345557     364.156624
    63   8320.0  1381.905722  1113.718064     362.151391
    64   8448.0  1389.809345  1123.332085     362.614899
    65   8576.0  1387.495488  1127.546355     363.303419
    66   8704.0  1387.559168  1135.279762     364.378238
    67   8832.0  1392.160261  1131.389935     365.169541
    68   8960.0  1384.284848  1139.148723     365.173092
    69   9088.0  1393.602888  1138.287790     366.955800
    70   9216.0  1401.508400  1144.242683     367.560515
    71   9344.0  1383.476464  1416.556506     367.770079
    72   9472.0  1399.756371  1427.541718     368.855174
    73   9600.0  1395.644505  1430.828678     368.451353
    74   9728.0  1398.404377  1441.892012     370.046555
    75   9856.0  1395.416334  1436.441000     369.863893
    76   9984.0  1396.164282  1450.124433     370.733278
    77  10112.0  1402.492592  1454.447868     370.776953
    78  10240.0  1406.450479  1468.631818     371.634594
    79  10368.0  1411.885734  1461.061477     370.303490
    80  10496.0  1406.787609  1464.915781     370.364179
    81  10624.0  1408.042500  1466.840291     370.418844
    82  10752.0  1391.520927  1471.962404     371.418350
    83  10880.0  1392.905317  1479.803306     371.093584
    84  11008.0  1418.291806  1479.445617     372.350368
    85  11136.0  1413.976363  1482.951387     373.112378
    86  11264.0  1408.236093  1483.728803     373.156775
    87  11392.0  1418.748427  1490.534828     374.005919
    88  11520.0  1413.245009  1496.566900     373.543848
    89  11648.0  1418.367787  1500.942204     374.600184
    90  11776.0  1427.736546  1504.294540     374.819581
    91  11904.0  1426.202480  1508.700423     375.557074
    92  12032.0  1412.218932  1509.216072     376.292210
    93  12160.0  1411.456176  1514.888270     375.917066
    94  12288.0  1421.828504  1417.898936     375.856971
    95  12416.0  1430.617052  1393.832667     374.840318
    96  12544.0  1441.405622  1390.500768     375.674313
    97  12672.0  1430.139018  1388.112750     375.453473




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.063 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
