
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   480.829468   706.883979     208.337443
    1     384.0   665.159184   836.949331     263.912257
    2     512.0   815.097181   921.095184     302.565263
    3     640.0   913.776671   929.546113     329.934557
    4     768.0   982.085196   989.660727     350.149820
    5     896.0  1040.799890  1028.838198     355.900705
    6    1024.0  1083.613500  1081.054304     354.646683
    7    1152.0  1100.725550  1075.407043     349.489066
    8    1280.0  1136.082977  1102.809238     350.013666
    9    1408.0  1169.546206  1140.327694     340.749964
    10   1536.0  1199.219277  1156.809474     332.792117
    11   1664.0  1216.891753  1192.225307     329.186972
    12   1792.0  1232.441364  1194.212467     325.967129
    13   1920.0  1264.002062  1225.369983     325.073153
    14   2048.0  1269.023529  1250.864590     324.536058
    15   2176.0  1238.641693   966.121437     325.779801
    16   2304.0  1254.496787  1000.912344     326.040109
    17   2432.0  1278.011242  1040.166451     326.928375
    18   2560.0  1281.804345  1070.834193     327.985748
    19   2688.0  1293.209688  1096.757681     329.622846
    20   2816.0  1308.077391  1128.332105     329.647402
    21   2944.0  1313.910620  1149.675438     331.398095
    22   3072.0  1326.382690  1174.457402     333.698107
    23   3200.0  1337.392371  1172.080281     335.307293
    24   3328.0  1350.145086  1204.275332     336.059193
    25   3456.0  1353.698196  1224.677502     337.369317
    26   3584.0  1366.050808  1251.062771     338.302653
    27   3712.0  1365.251596  1265.008668     340.757936
    28   3840.0  1377.816750  1286.597917     340.638767
    29   3968.0  1372.242531  1300.729783     341.039182
    30   4096.0  1388.602465  1319.494675     338.647788
    31   4224.0  1325.904730  1282.628039     342.589944
    32   4352.0  1343.646934  1305.098042     345.425381
    33   4480.0  1342.563184  1322.946985     345.966215
    34   4608.0  1359.607841  1336.898454     347.286958
    35   4736.0  1362.256722  1348.933894     348.089870
    36   4864.0  1373.185981  1361.215895     349.303667
    37   4992.0  1369.723458  1376.431767     350.140417
    38   5120.0  1381.242754  1383.196435     351.005294
    39   5248.0  1373.802971  1347.244869     351.871960
    40   5376.0  1379.692835  1362.117734     351.912574
    41   5504.0  1379.180530  1378.754783     354.037913
    42   5632.0  1391.226927  1394.648967     353.213060
    43   5760.0  1398.871940  1403.943297     354.842191
    44   5888.0  1391.723312  1406.450467     354.711777
    45   6016.0  1403.031297  1426.199094     356.582762
    46   6144.0  1409.913279  1429.633161     356.797199
    47   6272.0  1406.351546  1400.803400     357.830473
    48   6400.0  1411.939549  1410.375244     358.992603
    49   6528.0  1414.180686  1412.526297     359.222628
    50   6656.0  1418.061993  1427.692479     359.519540
    51   6784.0  1416.745782  1435.061150     360.703475
    52   6912.0  1425.017072  1441.272428     360.647525
    53   7040.0  1421.514817  1449.835216     360.906291
    54   7168.0  1425.024576  1457.153134     361.971368
    55   7296.0  1423.116375  1086.373983     362.118156
    56   7424.0  1427.505549  1097.731147     362.999048
    57   7552.0  1428.661706  1110.900969     363.650592
    58   7680.0  1430.691894  1121.549278     363.500590
    59   7808.0  1430.540582  1132.794534     364.366845
    60   7936.0  1433.844307  1142.277522     364.723191
    61   8064.0  1431.911701  1148.015982     364.867869
    62   8192.0  1430.911530  1153.353752     364.269578
    63   8320.0  1383.967195  1116.697393     361.609600
    64   8448.0  1386.915855  1125.047432     362.495551
    65   8576.0  1385.808916  1128.245216     363.343503
    66   8704.0  1380.151692  1132.888151     364.298065
    67   8832.0  1392.548272  1132.600073     364.924892
    68   8960.0  1386.145217  1140.168758     365.900970
    69   9088.0  1393.868137  1138.011909     366.866778
    70   9216.0  1404.785347  1144.220978     366.895449
    71   9344.0  1396.301536  1420.801780     367.921056
    72   9472.0  1396.993813  1428.146324     369.068623
    73   9600.0  1400.421367  1428.439080     368.685716
    74   9728.0  1400.693567  1436.115898     369.499753
    75   9856.0  1400.328876  1441.537123     369.757700
    76   9984.0  1392.068613  1449.214771     370.631252
    77  10112.0  1402.764047  1455.927190     371.410889
    78  10240.0  1412.861214  1464.536898     371.510407
    79  10368.0  1415.739616  1458.099672     369.500843
    80  10496.0  1404.506649  1466.288188     370.137921
    81  10624.0  1406.398425  1469.409807     370.313347
    82  10752.0  1395.123384  1468.617014     371.000250
    83  10880.0  1391.536369  1478.724120     371.780450
    84  11008.0  1419.855638  1475.617185     372.505217
    85  11136.0  1419.753159  1485.827661     372.894977
    86  11264.0  1410.432775  1486.195901     372.784156
    87  11392.0  1422.640015  1491.076665     374.712934
    88  11520.0  1413.381629  1494.714537     373.583464
    89  11648.0  1419.519744  1500.680115     374.950412
    90  11776.0  1434.068333  1499.828856     374.824019
    91  11904.0  1429.766791  1508.343024     375.285154
    92  12032.0  1415.552645  1509.599546     375.969035
    93  12160.0  1414.551275  1516.703042     375.899351
    94  12288.0  1428.067047  1421.564446     375.932388
    95  12416.0  1438.280824  1396.340172     374.700443
    96  12544.0  1443.182172  1395.914840     375.647946
    97  12672.0  1430.568101  1391.404704     375.387475




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.161 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
