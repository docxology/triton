
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   479.574659   707.853509     207.610583
    1     384.0   657.751832   828.481171     263.004184
    2     512.0   799.482146   915.989563     303.497332
    3     640.0   884.283241   926.294912     331.707191
    4     768.0   963.500478   995.435333     350.945542
    5     896.0  1021.479459  1041.480676     357.584571
    6    1024.0  1060.686983  1071.380556     356.422367
    7    1152.0  1098.452973  1067.367306     350.765959
    8    1280.0  1135.831622  1111.970351     350.275547
    9    1408.0  1168.093867  1141.563105     341.566591
    10   1536.0  1199.275863  1166.733779     333.046369
    11   1664.0  1213.811629  1192.891253     330.913215
    12   1792.0  1237.153086  1195.603803     325.811551
    13   1920.0  1261.726048  1219.538320     324.960136
    14   2048.0  1275.739196  1252.072916     324.662621
    15   2176.0  1239.756542   963.575395     325.795739
    16   2304.0  1247.401596  1007.471511     326.011697
    17   2432.0  1266.219324  1034.802869     327.329885
    18   2560.0  1279.955659  1073.495205     328.490274
    19   2688.0  1294.088916  1104.324746     329.946988
    20   2816.0  1306.374769  1127.072430     329.858980
    21   2944.0  1319.962865  1147.405146     331.669875
    22   3072.0  1316.557745  1175.190582     333.882942
    23   3200.0  1336.405668  1174.819129     335.425756
    24   3328.0  1344.001978  1201.370977     336.285632
    25   3456.0  1355.318389  1229.792550     337.280020
    26   3584.0  1355.900918  1248.547407     338.875289
    27   3712.0  1367.836375  1265.487800     340.375308
    28   3840.0  1366.098296  1280.032747     340.132632
    29   3968.0  1375.268886  1297.818259     341.451137
    30   4096.0  1387.154294  1320.508883     338.554215
    31   4224.0  1327.991526  1278.122740     343.114104
    32   4352.0  1339.705318  1304.189221     345.322726
    33   4480.0  1339.470116  1316.486773     345.848102
    34   4608.0  1353.596342  1338.604627     346.775591
    35   4736.0  1353.041888  1343.202215     348.228627
    36   4864.0  1361.091238  1357.246434     349.141178
    37   4992.0  1362.088341  1371.682375     350.396427
    38   5120.0  1369.841091  1388.600760     351.308240
    39   5248.0  1372.838896  1348.883510     351.953573
    40   5376.0  1369.472332  1373.516552     351.646521
    41   5504.0  1371.242253  1375.453608     353.506148
    42   5632.0  1387.178439  1399.577323     353.031961
    43   5760.0  1384.404303  1402.617759     355.238274
    44   5888.0  1387.679228  1419.737444     355.134280
    45   6016.0  1393.362854  1417.074201     356.666504
    46   6144.0  1403.696982  1437.059791     357.037863
    47   6272.0  1405.951696  1396.964066     357.858214
    48   6400.0  1406.479394  1417.026209     358.595119
    49   6528.0  1408.357736  1408.868530     359.508669
    50   6656.0  1410.752659  1437.546793     359.857461
    51   6784.0  1411.169884  1433.954203     360.473112
    52   6912.0  1418.696911  1439.025214     360.872775
    53   7040.0  1418.763241  1444.045438     361.122093
    54   7168.0  1420.165996  1456.442803     361.883126
    55   7296.0  1421.987687  1086.780202     362.259767
    56   7424.0  1423.734730  1099.794872     363.335791
    57   7552.0  1426.087390  1111.625271     363.500170
    58   7680.0  1430.127420  1123.562516     363.903144
    59   7808.0  1426.414262  1133.808314     364.780592
    60   7936.0  1430.969195  1142.238459     364.704971
    61   8064.0  1428.040926  1150.345878     364.967456
    62   8192.0  1425.197164  1153.748339     364.362070
    63   8320.0  1384.827585  1116.402426     361.903035
    64   8448.0  1383.421850  1125.700233     362.557425
    65   8576.0  1385.002763  1126.676314     363.544054
    66   8704.0  1385.128742  1133.078551     364.311423
    67   8832.0  1393.306444  1133.949178     365.218512
    68   8960.0  1383.813911  1140.691624     365.932104
    69   9088.0  1397.197822  1137.128688     367.062685
    70   9216.0  1400.929131  1145.335086     367.418245
    71   9344.0  1388.794577  1418.959059     367.854492
    72   9472.0  1401.558597  1431.561555     368.482234
    73   9600.0  1403.065073  1432.954527     369.163541
    74   9728.0  1399.780810  1440.402151     369.853645
    75   9856.0  1396.897215  1439.214504     370.129641
    76   9984.0  1396.254927  1448.150354     370.693367
    77  10112.0  1406.060153  1450.999086     371.282154
    78  10240.0  1406.825977  1468.182586     371.191455
    79  10368.0  1409.523082  1460.362033     370.419074
    80  10496.0  1407.118371  1465.997870     370.328670
    81  10624.0  1407.540713  1465.479166     370.486300
    82  10752.0  1391.249303  1469.189668     371.305442
    83  10880.0  1392.354392  1481.178941     371.502014
    84  11008.0  1412.471388  1476.864492     373.019345
    85  11136.0  1410.852707  1484.455019     372.890544
    86  11264.0  1409.586532  1487.066250     373.011712
    87  11392.0  1414.828581  1493.344565     374.133868
    88  11520.0  1407.842172  1495.983968     373.640702
    89  11648.0  1417.983446  1498.971139     375.141319
    90  11776.0  1427.771816  1500.899142     374.935012
    91  11904.0  1426.837882  1507.219758     375.682023
    92  12032.0  1411.115130  1509.676028     375.995578
    93  12160.0  1407.038389  1514.865039     376.005666
    94  12288.0  1421.605203  1418.001082     376.305478
    95  12416.0  1432.687137  1393.194275     374.936175
    96  12544.0  1435.050959  1393.432189     375.784219
    97  12672.0  1428.061643  1390.900894     375.176452




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.261 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
