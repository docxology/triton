#!/usr/bin/env python3
"""
Triton Usage Validation Script

Comprehensive validation that all implementations in the active inference framework
use real Triton methods and GPU acceleration. This script verifies:

1. All Triton kernels are properly implemented (not just placeholders)
2. GPU acceleration is used when available
3. PyTorch fallbacks work correctly when Triton is unavailable
4. Memory management uses real Triton features
5. Performance optimizations leverage Triton capabilities
6. No mock methods are used in production code

Usage:
    python validate_triton_usage.py
"""

import sys
import os
import torch
import inspect
import importlib
from pathlib import Path
from typing import Dict, List, Any, Tuple

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))


def validate_triton_imports():
    """Validate that all Triton imports are correctly handled."""
    print("üîç VALIDATING TRITON IMPORTS")
    print("=" * 50)

    validation_results = {
        "triton_available": False,
        "imports_correct": False,
        "fallbacks_implemented": False,
        "gpu_acceleration": False,
    }

    try:
        # Test basic Triton availability
        try:
            import triton
            import triton.language as tl

            validation_results["triton_available"] = True
            print("‚úì Triton is available")
        except ImportError:
            print("‚ö†Ô∏è  Triton not available - using PyTorch fallbacks")
            triton = None
            tl = None

        # Test our imports
        from core import TRITON_AVAILABLE, TritonFeatureManager, GPUAccelerator

        if TRITON_AVAILABLE == (triton is not None):
            validation_results["imports_correct"] = True
            print("‚úì Triton availability correctly detected")
        else:
            print("‚úó Triton availability detection mismatch")

        # Test GPU acceleration
        fm = TritonFeatureManager()
        ga = GPUAccelerator(fm)

        device = ga.device
        if device.type in ["cuda", "mps"]:
            validation_results["gpu_acceleration"] = True
            print(f"‚úì GPU acceleration enabled: {device.type.upper()}")
        else:
            print("‚ö†Ô∏è  Using CPU - GPU acceleration not available")

        # Test fallback functionality
        if not TRITON_AVAILABLE:
            # Verify fallback implementations exist
            try:
                from core import vector_add_kernel, matrix_multiply_kernel

                # Try to call fallback functions
                a = torch.randn(10)
                b = torch.randn(10)
                c = torch.zeros(10)
                vector_add_kernel(a, b, c, 10)
                validation_results["fallbacks_implemented"] = True
                print("‚úì PyTorch fallbacks working correctly")
            except Exception as e:
                print(f"‚úó Fallback implementation error: {e}")

    except Exception as e:
        print(f"‚úó Import validation failed: {e}")
        import traceback

        traceback.print_exc()

    return validation_results


def validate_kernel_implementations():
    """Validate that all kernel implementations are real (not placeholders)."""
    print("\nüî¨ VALIDATING KERNEL IMPLEMENTATIONS")
    print("=" * 50)

    validation_results = {
        "kernels_found": [],
        "real_implementations": [],
        "placeholder_detected": [],
        "triton_features_used": [],
    }

    try:
        from core import TRITON_AVAILABLE

        if TRITON_AVAILABLE:
            # Check Triton kernel implementations
            triton_kernel_functions = [
                "vector_add_kernel",
                "matrix_multiply_kernel",
                "softmax_kernel",
                "layer_norm_kernel",
                "attention_kernel",
                "conv2d_kernel",
            ]

            for kernel_name in triton_kernel_functions:
                try:
                    # Import and inspect the kernel
                    module = importlib.import_module("core")
                    kernel_func = getattr(module, kernel_name)

                    # Check if it's a real Triton kernel
                    source = inspect.getsource(kernel_func)

                    if "@triton.jit" in source:
                        validation_results["kernels_found"].append(kernel_name)
                        validation_results["real_implementations"].append(kernel_name)

                        # Check for Triton-specific features
                        triton_features = []
                        if "tl.program_id" in source:
                            triton_features.append("program_id")
                        if "tl.load" in source:
                            triton_features.append("load")
                        if "tl.store" in source:
                            triton_features.append("store")
                        if "tl.arange" in source:
                            triton_features.append("arange")
                        if "BLOCK_SIZE" in source:
                            triton_features.append("block_size")

                        if triton_features:
                            validation_results["triton_features_used"].extend(
                                triton_features
                            )

                        print(
                            f"‚úì Real Triton kernel: {kernel_name} ({len(triton_features)} features)"
                        )
                    else:
                        validation_results["placeholder_detected"].append(kernel_name)
                        print(f"‚ö†Ô∏è  Placeholder detected: {kernel_name}")

                except Exception as e:
                    print(f"‚úó Error checking kernel {kernel_name}: {e}")

        # Check other modules for Triton kernels
        modules_to_check = ["free_energy", "message_passing", "pomdp_active_inference"]

        for module_name in modules_to_check:
            try:
                module = importlib.import_module(module_name)
                source = inspect.getsource(module)

                triton_kernel_count = source.count("@triton.jit")
                if triton_kernel_count > 0:
                    print(
                        f"‚úì {module_name}: {triton_kernel_count} Triton kernels found"
                    )
                    validation_results["kernels_found"].append(f"{module_name}_kernels")
                else:
                    print(f"‚ö†Ô∏è  {module_name}: No Triton kernels found")

            except Exception as e:
                print(f"‚úó Error checking module {module_name}: {e}")

    except Exception as e:
        print(f"‚úó Kernel validation failed: {e}")

    return validation_results


def validate_gpu_acceleration():
    """Validate that GPU acceleration is properly implemented."""
    print("\nüöÄ VALIDATING GPU ACCELERATION")
    print("=" * 50)

    validation_results = {
        "gpu_available": False,
        "memory_management": False,
        "kernel_launch": False,
        "synchronization": False,
        "performance_better": False,
    }

    try:
        # Check GPU availability
        if torch.cuda.is_available():
            validation_results["gpu_available"] = True
            print("‚úì CUDA GPU available")
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            validation_results["gpu_available"] = True
            print("‚úì MPS GPU available")
        else:
            print("‚ö†Ô∏è  No GPU available - using CPU")

        # Test memory management
        from core import GPUAccelerator, TritonFeatureManager

        fm = TritonFeatureManager()
        ga = GPUAccelerator(fm)

        # Test tensor allocation
        try:
            tensors = ga.allocate_tensors([(100, 100), (50, 200), (10, 10, 10)])
            validation_results["memory_management"] = len(tensors) == 3
            print("‚úì Memory management working")
        except Exception as e:
            print(f"‚úó Memory management error: {e}")

        # Test synchronization
        try:
            ga.synchronize()
            validation_results["synchronization"] = True
            print("‚úì GPU synchronization working")
        except Exception as e:
            print(f"‚ö†Ô∏è  Synchronization error: {e}")

        # Test kernel launch (if available)
        from core import TRITON_AVAILABLE, create_triton_kernel

        if TRITON_AVAILABLE and validation_results["gpu_available"]:
            try:
                kernel = create_triton_kernel("vector_add")
                if kernel is not None:
                    validation_results["kernel_launch"] = True
                    print("‚úì Kernel launch capability confirmed")
            except Exception as e:
                print(f"‚ö†Ô∏è  Kernel launch error: {e}")

        # Performance comparison
        if validation_results["gpu_available"]:
            try:
                # Simple performance test
                size = 1000
                device = torch.device("cuda" if torch.cuda.is_available() else "mps")

                # GPU computation
                a_gpu = torch.randn(size, device=device)
                b_gpu = torch.randn(size, device=device)

                import time

                start_time = time.time()
                for _ in range(100):
                    c_gpu = a_gpu + b_gpu
                gpu_time = time.time() - start_time

                # CPU computation
                a_cpu = a_gpu.cpu()
                b_cpu = b_gpu.cpu()
                start_time = time.time()
                for _ in range(100):
                    c_cpu = a_cpu + b_cpu
                cpu_time = time.time() - start_time

                if gpu_time < cpu_time:
                    validation_results["performance_better"] = True
                    speedup = cpu_time / gpu_time
                    print(f"‚úì GPU speedup confirmed: {speedup:.1f}x faster")
                else:
                    slowdown = gpu_time / cpu_time
                    print(f"‚ö†Ô∏è  GPU slower than CPU: {slowdown:.1f}x (overhead likely)")

            except Exception as e:
                print(f"‚ö†Ô∏è  Performance test error: {e}")

    except Exception as e:
        print(f"‚úó GPU acceleration validation failed: {e}")

    return validation_results


def validate_real_vs_mock_methods():
    """Validate that no mock methods are used in production code."""
    print("\nüé≠ VALIDATING REAL VS MOCK METHODS")
    print("=" * 50)

    validation_results = {
        "mock_methods_detected": [],
        "real_implementations": [],
        "placeholder_functions": [],
        "test_only_code": [],
    }

    try:
        # Check for common mock indicators
        mock_indicators = [
            "placeholder",
            "mock",
            "dummy",
            "not implemented",
            "todo",
            "fixme",
            "hack",
            "temporary",
            "placeholder implementation",
        ]

        # Check all source files
        src_dir = Path(__file__).parent
        python_files = list(src_dir.glob("*.py"))

        for py_file in python_files:
            try:
                with open(py_file, "r") as f:
                    content = f.read().lower()

                # Check for mock indicators
                for indicator in mock_indicators:
                    if indicator in content:
                        validation_results["mock_methods_detected"].append(
                            {
                                "file": py_file.name,
                                "indicator": indicator,
                                "count": content.count(indicator),
                            }
                        )

                # Check for real implementation indicators
                real_indicators = [
                    "@triton.jit",
                    "tl.load",
                    "tl.store",
                    "torch.",
                    "def ",
                ]
                real_count = sum(
                    content.count(indicator) for indicator in real_indicators
                )

                if real_count > 0:
                    validation_results["real_implementations"].append(
                        {"file": py_file.name, "real_indicators": real_count}
                    )

                # Check for test-only code
                if "test" in py_file.name.lower() or "example" in py_file.name.lower():
                    validation_results["test_only_code"].append(py_file.name)

                print(
                    f"‚úì Checked {py_file.name}: {real_count} real implementation indicators"
                )

            except Exception as e:
                print(f"‚úó Error checking {py_file.name}: {e}")

        # Summary
        if validation_results["mock_methods_detected"]:
            print(
                f"\n‚ö†Ô∏è  Mock methods detected in {len(validation_results['mock_methods_detected'])} files"
            )
            for detection in validation_results["mock_methods_detected"]:
                print(
                    f"  - {detection['file']}: {detection['indicator']} ({detection['count']} times)"
                )
        else:
            print("\n‚úì No mock methods detected in production code")

        print(
            f"‚úì Real implementations found in {len(validation_results['real_implementations'])} files"
        )

    except Exception as e:
        print(f"‚úó Mock method validation failed: {e}")

    return validation_results


def validate_memory_optimization():
    """Validate that memory optimization uses real Triton features."""
    print("\nüíæ VALIDATING MEMORY OPTIMIZATION")
    print("=" * 50)

    validation_results = {
        "shared_memory_usage": False,
        "coalesced_access": False,
        "memory_pooling": False,
        "efficient_allocation": False,
    }

    try:
        # Check for shared memory usage in kernels
        kernel_files = ["core.py", "free_energy.py", "message_passing.py"]

        for file_name in kernel_files:
            try:
                file_path = Path(__file__).parent / file_name
                with open(file_path, "r") as f:
                    content = f.read()

                if "shared memory" in content.lower() or "tl.zeros" in content:
                    validation_results["shared_memory_usage"] = True
                    print(f"‚úì Shared memory usage found in {file_name}")

                if "coalesced" in content.lower() or "contiguous" in content:
                    validation_results["coalesced_access"] = True
                    print(f"‚úì Coalesced access patterns found in {file_name}")

            except Exception as e:
                print(f"‚ö†Ô∏è  Error checking {file_name}: {e}")

        # Test memory pooling
        from core import GPUAccelerator, TritonFeatureManager

        try:
            fm = TritonFeatureManager()
            ga = GPUAccelerator(fm)

            # Test repeated allocations (simulating memory pooling)
            for _ in range(10):
                tensors = ga.allocate_tensors([(100, 100), (50, 200)])
                # If no memory errors, allocation is efficient
                del tensors

            validation_results["efficient_allocation"] = True
            print("‚úì Efficient memory allocation confirmed")

        except Exception as e:
            print(f"‚ö†Ô∏è  Memory allocation test error: {e}")

        # Check for memory pooling indicators in code
        try:
            with open(Path(__file__).parent / "core.py", "r") as f:
                core_content = f.read()

            if "memory" in core_content.lower():
                validation_results["memory_pooling"] = True
                print("‚úì Memory management code found")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error checking memory pooling: {e}")

    except Exception as e:
        print(f"‚úó Memory optimization validation failed: {e}")

    return validation_results


def validate_performance_optimizations():
    """Validate that performance optimizations use real Triton features."""
    print("\n‚ö° VALIDATING PERFORMANCE OPTIMIZATIONS")
    print("=" * 50)

    validation_results = {
        "kernel_fusion": False,
        "vectorization": False,
        "parallelization": False,
        "pipelining": False,
        "autotuning": False,
    }

    try:
        # Check kernel implementations for optimization features
        files_to_check = ["core.py", "free_energy.py", "message_passing.py"]

        for file_name in files_to_check:
            try:
                file_path = Path(__file__).parent / file_name
                with open(file_path, "r") as f:
                    content = f.read()

                # Check for various optimization indicators
                if "vectorization" in content.lower() or "tl.arange" in content:
                    validation_results["vectorization"] = True
                    print(f"‚úì Vectorization found in {file_name}")

                if "parallel" in content.lower() or "program_id" in content:
                    validation_results["parallelization"] = True
                    print(f"‚úì Parallelization found in {file_name}")

                if "pipeline" in content.lower() or "async" in content:
                    validation_results["pipelining"] = True
                    print(f"‚úì Pipelining found in {file_name}")

                if "block_size" in content or "BLOCK_SIZE" in content:
                    validation_results["kernel_fusion"] = True
                    print(f"‚úì Block-level optimization found in {file_name}")

            except Exception as e:
                print(f"‚ö†Ô∏è  Error checking {file_name}: {e}")

        # Test autotuning capability
        from core import create_triton_kernel, TRITON_AVAILABLE

        if TRITON_AVAILABLE:
            try:
                # Test kernel creation with different configurations
                configs_tested = 0
                for block_size in [128, 256, 512]:
                    try:
                        kernel = create_triton_kernel(
                            "vector_add", block_size=block_size
                        )
                        if kernel is not None:
                            configs_tested += 1
                    except:
                        pass

                if configs_tested > 1:
                    validation_results["autotuning"] = True
                    print(
                        f"‚úì Autotuning capability confirmed ({configs_tested} configurations)"
                    )

            except Exception as e:
                print(f"‚ö†Ô∏è  Autotuning test error: {e}")

    except Exception as e:
        print(f"‚úó Performance optimization validation failed: {e}")

    return validation_results


def generate_validation_report(all_results: Dict[str, Any]):
    """Generate comprehensive validation report."""
    print("\nüìä GENERATING VALIDATION REPORT")
    print("=" * 50)

    report = {
        "timestamp": (
            torch.cuda.Event(enable_timing=True).elapsed_time(
                torch.cuda.Event(enable_timing=True)
            )
            if torch.cuda.is_available()
            else 0
        ),
        "validation_summary": {},
        "scores": {},
        "recommendations": [],
    }

    # Calculate scores for each validation category
    scores = {}
    total_score = 0
    categories_checked = 0

    for category, results in all_results.items():
        if isinstance(results, dict):
            # Calculate category score based on boolean values
            boolean_values = [v for v in results.values() if isinstance(v, bool)]
            if boolean_values:
                category_score = sum(boolean_values) / len(boolean_values)
                scores[category] = category_score
                total_score += category_score
                categories_checked += 1

    if categories_checked > 0:
        overall_score = total_score / categories_checked
        report["overall_score"] = overall_score

        if overall_score >= 0.9:
            report["assessment"] = "EXCELLENT: All validations passed"
        elif overall_score >= 0.7:
            report["assessment"] = "GOOD: Core functionality validated"
        elif overall_score >= 0.5:
            report["assessment"] = "FAIR: Some issues need attention"
        else:
            report["assessment"] = "NEEDS IMPROVEMENT: Significant issues found"

    # Generate recommendations
    recommendations = []

    # Check Triton availability
    triton_results = all_results.get("triton_imports", {})
    if not triton_results.get("triton_available", False):
        recommendations.append("Install Triton for GPU acceleration")

    # Check for mock methods
    mock_results = all_results.get("real_vs_mock", {})
    if mock_results.get("mock_methods_detected", []):
        recommendations.append("Remove or implement placeholder functions")

    # Check GPU acceleration
    gpu_results = all_results.get("gpu_acceleration", {})
    if not gpu_results.get("gpu_available", False):
        recommendations.append("Ensure GPU is available for optimal performance")

    # Check kernel implementations
    kernel_results = all_results.get("kernel_implementations", {})
    if not kernel_results.get("real_implementations", []):
        recommendations.append("Implement real Triton kernels (not just placeholders)")

    report["recommendations"] = recommendations

    # Print summary
    print("\nVALIDATION SUMMARY:")
    print(f"Overall Score: {overall_score:.1f}")
    print(f"Assessment: {report['assessment']}")

    if recommendations:
        print("\nRECOMMENDATIONS:")
        for rec in recommendations:
            print(f"‚Ä¢ {rec}")

    return report


def main():
    """Run comprehensive Triton usage validation."""
    print("üß™ TRITON USAGE VALIDATION")
    print("=" * 60)
    print(
        "Validating that all implementations use real Triton methods and GPU acceleration..."
    )

    all_results = {}

    try:
        # Run all validations
        all_results["triton_imports"] = validate_triton_imports()
        all_results["kernel_implementations"] = validate_kernel_implementations()
        all_results["gpu_acceleration"] = validate_gpu_acceleration()
        all_results["real_vs_mock"] = validate_real_vs_mock_methods()
        all_results["memory_optimization"] = validate_memory_optimization()
        all_results["performance_optimizations"] = validate_performance_optimizations()

        # Generate report
        report = generate_validation_report(all_results)

        # Save detailed results
        import json

        output_dir = Path(__file__).parent / "outputs"
        output_dir.mkdir(exist_ok=True)

        with open(output_dir / "triton_validation_report.json", "w") as f:
            json.dump(
                {"validation_results": all_results, "report": report},
                f,
                indent=2,
                default=str,
            )

        print(
            f"\nüíæ Detailed validation report saved to: {output_dir}/triton_validation_report.json"
        )

        # Final assessment
        overall_score = report.get("overall_score", 0)
        if overall_score >= 0.7:
            print("\nüéâ VALIDATION SUCCESSFUL!")
            print("‚úì All implementations use real Triton methods")
            print("‚úì GPU acceleration properly implemented")
            print("‚úì No mock methods in production code")
            print("‚úì Memory and performance optimizations active")
        else:
            print("\n‚ö†Ô∏è  VALIDATION ISSUES FOUND")
            print("Some components may need attention - check recommendations above")

        return overall_score >= 0.7

    except Exception as e:
        print(f"\nüí• VALIDATION FAILED: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
